#!/usr/bin/env python3
# coding: utf-8

import json
from datetime import datetime, timezone

# --- Imports des scrapers ---
from scrapers import confort_moderne


def main():
    all_events = []

    # --- CONFORT MODERNE ---
    print("\nüé∏ CONFORT MODERNE...")
    try:
        confort_events = confort_moderne.scrape_confort_moderne()
        print(f"‚úÖ {len(confort_events)} √©v√©nements r√©cup√©r√©s depuis le Confort Moderne.")
        all_events += confort_events
    except Exception as e:
        print(f"‚ùå Erreur lors du scraping Confort Moderne : {e}")

    # --- Nettoyage des doublons ---
    seen = set()
    unique = []
    for ev in all_events:
        key = (
            ev.get("title", "").strip().lower(),
            ev.get("source", "").strip().lower(),
        )
        if key not in seen:
            seen.add(key)
            unique.append(ev)

    # --- Tri chronologique robuste ---
    def parse_date(value):
        if not value:
            return datetime.max
        try:
            dt = datetime.fromisoformat(value.replace("Z", "+00:00"))
            if dt.tzinfo is not None:
                dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            return datetime.max

    def sort_key(ev):
        return parse_date(ev.get("release")) or parse_date(ev.get("date"))

    unique.sort(key=sort_key)

    # --- Sauvegarde ---
    output = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "events": unique,
    }

    with open("events.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(
        f"\nüíæ {len(unique)} √©v√©nements sauvegard√©s dans events.json "
        f"({len(all_events)} collect√©s avant d√©doublonnage)"
    )

    # --- R√©sum√© final ---
    print(f"   üé∏ Confort Moderne : {len(locals().get('confort_events', []))}")


if __name__ == "__main__":
    main()
